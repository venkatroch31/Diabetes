<html>
  <head>
    <style>
h1 {
  color: blue;
  font-family: verdana;
  font-size: 300%;
}
h2 {
  color: black;
  font-family: verdana;
  font-size: 170%;
}
p {
  color: black;
  font-family: verdana;
  font-size: 100%;
}
ul {
  color: black;
  font-family: verdana;
  font-size: 90%;
}
  div {
  padding-top: 10px;
  padding-right: 40px;
  padding-bottom: 50px;
  padding-left: 100px;
}
</style>
  </head>
  
  
<body><div>
  <h1><center>Machine Learning for pima Indian Diabetes</center></h1>
 
  <h2>Introduction</h2>
  <h4> What is Diabetes?</h4>
  <p>  Diabetes is a disease which occurs when the blood glucose level becomes high, which 
       ultimately leads to other health problems such as heart diseases, kidney disease etc. 
       Diabetes is caused mainly due to the consumption of highly processed food, bad 
       consumption habits etc. According to WHO, the number of people with diabetes has 
       been increased over the years. 
  </p>
  <p>The Pima are a group of Native Americans living in Arizona. A genetic predisposition 
    allowed this group to survive normally to a diet poor of carbohydrates for years. In the 
    recent years, because of a sudden shift from traditional agricultural crops to processed 
    foods, together with a decline in physical activity, made them develop the highest prevalence
    of type 2 diabetes and for this reason they have been subject of many studies. 
    
    
  </p>
  <h2>Description </h2>
  <p>
    <strong>Source:</strong> </p>
  <p>
    <strong>Data:</strong> Download diabetes.zip from Kaggle.
  </p>
   <p>
    <strong>Problem statement :</strong>
 
    The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, 
    based on certain diagnostic measurements included in the dataset. 
  </p>
  
  
  
  <h2>Prerequisites</h2>
  <ul>
         <li>Python 3</li>
         <li>Anaconda (Scikit Learn, Numpy, Pandas, Matplotlib, Seaborn)</li>
         <li>Jupyter Notebook</li>
         <li>Basic understanding of supervised machine learning methods : specifically classification</li>
      </ul>



<h2> Data </h2>
<p>
  Outcome is the feature we are going to predict, 0 means no diabetes, 
  1 means diabetes, these 768 data points, 500 are labeled as 0 and 268 as 1
</p>
<center><img src="vis0.png" alt=" "> </center>
<center>
  <img src="vis.png" alt=" "> </center>
<ul>
         <li>Pregnancies: Number of times pregnant</li>
         <li>Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test</li>
         <li>BloodPressure: Diastolic blood pressure (mm Hg)</li>
         <li>SkinThickness: Triceps skin fold thickness (mm)</li>
          <li>Insulin: 2-Hour serum insulin (mu U/ml)</li>
          <li>BMI: Body mass index (weight in kg/(height in m)^2)</li>
          <li>DiabetesPedigreeFunction: Diabetes pedigree function</li>
          <li>Age: Age (years)</li>
          <li>Outcome: Class variable (0 or 1)</li>
      </ul>
  <h2>Train, CV and Test Datasets</h2>
  <p>
    Split the dataset randomly into three parts train, cross validation and test with 64%,16%, 20% of data respectively
  </p>
  <h2>Machine Learning Models</h2>
   <p>The Log Loss metric takes into account the probabilities underlying your models, and 
    not only the final output of the classification. The bolder the probabilities, the better
    will be your Log Loss — closer to zero. It is a measure of uncertainty, so a low Log Loss 
    means a low uncertainty/entropy of your model. Log Loss is similar to the Accuracy, but it 
    will favor models that distinguish more strongly the classes. Log Loss it useful to compare 
     models not only on their output but on their probabilistic outcome.</p>
  <h2>k-Nearest Neighbors</h2>
  <p>
    The k-NN algorithm is arguably the simplest machine learning algorithm. Building the model 
    consists only of storing the training data set. To make a prediction for a new data point, 
    the algorithm finds the closest data points in the training data set — its "nearest neighbors"
  </p>
  <center><img src="knn1.png" alt=" "> </center><br></br>
  <center><img src="knn2.png" alt=" "> </center>
<h2>Naive Bayes</h2>
<p>
  Naive Bayes is one of the most common classification algorithms.
</p>
<h2>Logistic Regresion</h2>
<p>
  Logistic Regresion is one of the most common classification algorithms.
</p>
<h2>Linear SVM</h2>
<p>
  Linear SVM is one of the most common classification algorithms.
</p>
<h2>Random Forest</h2>
<p>
  Random Forest is one of the most common classification algorithms.
</p>
<h2>XgBoost</h2>
<p>
  XgBoost is one of the most common classification algorithms.
</p>
  </div>
</body>
</html>
