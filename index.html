<html>
	<head>
		<style>
			div {
  			padding-top: 10px;
  			padding-right:200px;
  			padding-bottom: 50px;
  			padding-left: 200px;  
			align:left;
			
			}
			
		</style>
	</head>
	
	<body>
		<center><img src="credit-card.jpg" /></center>
		<div>
			<h1><center>Pima Indian Diabetes</center></h1>
			<h2>Introduction</h2>
			<p>
				The Pima are a group of Native Americans living in Arizona. A genetic predisposition allowed this group to survive normally to a diet poor of carbohydrates for years. In the recent years, because of a sudden shift from traditional agricultural crops to processed foods, together with a decline in physical activity, made them develop the highest prevalence of type 2 diabetes and for this reason they have been subject of many studies.
			</p>
			
			<p>
				Diabetes is a disease which occurs when the blood glucose level becomes high, which 
       				ultimately leads to other health problems such as heart diseases, kidney disease etc. 
       					Diabetes is caused mainly due to the consumption of highly processed food, bad 
       					consumption habits etc. According to WHO, the number of people with diabetes has 
       						been increased over the years. 

			</p>
			<h2>Dataset</h2>
			<p>
				Download the dataset from <a href="https://www.kaggle.com/uciml/pima-indians-diabetes-database/downloads/pima-indians-diabetes-database.zip/1">kaggle</a> 
			</p>
			<p>
				The dataset includes data from 768 women with 8 characteristics, in particular:
				<ul>
					<li>Pregnancies: Number of times pregnant</li>
					<li>Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test</li>
					<li>BloodPressure: Diastolic blood pressure (mm Hg)</li>
					<li>SkinThickness: Triceps skin fold thickness (mm)</li>
					<li>Insulin: 2-Hour serum insulin (mu U/ml)</li>
					<li>BMI: Body mass index (weight in kg/(height in m)^2)</li>
					<li>DiabetesPedigreeFunction: Diabetes pedigree function</li>
					<li>Age: Age (years)</li>
					<li>Outcome: Class variable (0 or 1)</li>
					
				</ul>
				The last column 'Outcome' of the dataset indicates if the person has been diagnosed with diabetes (1) or not (0)
			</p>
			<h2>Problem</h2>
			<p>
				The type of dataset and problem is a classic supervised binary classification. Given a number of elements all with certain characteristics (features), we want to build a machine learning model to identify people affected by type 2 diabetes.
			</p>
			<p>
				To solve the problem we will have to analyse the data, do any required transformation and normalisation, apply a machine learning algorithm, train a model, check the performance of the trained model and iterate with other algorithms until we find the most performant for our type of dataset.
			</p>
			<h2>Setup</h2>
			
			<p>
				Import neceessary libraries. Let‚Äôs begin:
			</p>
		
			<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					import pandas as pd</br>
					import matplotlib.pyplot as plt</br>
					import re</br>
					import numpy as np</br>
					import seaborn as sns</br>
					import time</br>
					import warnings</br>
					import sqlite3</br>
					warnings.filterwarnings("ignore")</br>
					from nltk.corpus import stopwords</br>
					from sklearn.preprocessing import normalize</br>
					from sklearn.feature_extraction.text import CountVectorizer</br>
					from sklearn.manifold import TSNE</br>
					from sklearn.neighbors import KNeighborsClassifier</br>
					from sklearn.metrics import confusion_matrix</br>
					from sklearn.metrics.classification import accuracy_score, log_loss</br>
					from sklearn.svm import SVC</br>
					from sklearn.calibration import CalibratedClassifierCV</br>
					from sklearn.model_selection import train_test_split</br>
					from sklearn.model_selection import GridSearchCV</br>
					from sklearn.metrics import normalized_mutual_info_score</br>
					from sklearn.ensemble import RandomForestClassifier</br>
					from sklearn import metrics</br>
					from nltk.stem.porter import PorterStemmer</br>

				</code>
			</div>
			<h2>Load the dataset</h2>
			<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					diabetes = pd.read_csv('diabetes.csv')</br>
					print('\nFeatures : ', diabetes.columns.values)</br>
					Features :  ['Pregnancies' 'Glucose' 'BloodPressure' 'SkinThickness' 'Insulin' 'BMI'
 						'DiabetesPedigreeFunction' 'Age' 'Outcome']</br>
					

				</code>
			</div>
			<h2>Data correlation matrix</h2>
			<p>
				The correlation matrix is an important tool to understand the correlation between the different characteristics. The values range from -1 to 1 and the closer a value is to 1 the bettere correlation there is between two characteristics. Let's calculate the correlation matrix for our dataset.
			</p>
			<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					plt.figure(figsize=(12,10))</br>
					p=sns.heatmap(diabetes.corr(), annot=True,cmap ='RdYlGn')  </br>
					

				</code>
			</div>
			<h2>Visualise the Data</h2>
			<p>
				Visualising the data is an important step of the data analysis. With a graphical visualisation of the data we have a better understanding of the various features values distribution: for example we can understand what's the average age of the people or the average BMI etc...
			</p>
			<center><img src=""/></center>
			<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					df = diabetes.hist(figsize=(12,10))</br>					

				</code>
			</div>
			
			
				<p>
					The following columns or variables have an invalid zero value:
					<ul>
						<li>Glucose</li>
						<li>BloodPressure</li>
						<li>SkinThickness</li>
						<li>Insulin</li>
						<li>BMI</li>
					</ul>
					
				</p>
				<p>
					
					It is better to replace zeros with nan since after that counting them would be easier and zeros need to be replaced with suitable values
					
				</p>
				<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
					<code>
					
						diabetes_copy = diabetes.copy(deep = True)</br>
						diabetes_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)
						</br>
					
						print(diabetes_copy.isnull().sum())</br>					

					</code>
				</div>
				
				<div align="left" style="padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
					<code>
					
						Pregnancies                   0</br>
						Glucose                       5</br>					
						BloodPressure                35</br>
						SkinThickness               227</br>
						Insulin                     374</br>
						BMI                          11</br>
						DiabetesPedigreeFunction      0</br>
						Age                           0</br>
						Outcome                       0</br>
						dtype: int64

					</code>
				</div>

			<h2>Data cleaning and transformation</h2>
			<p>
				We have noticed from the previous analysis that some patients have missing data for some of the features. Machine learning algorithms don't work very well when the data is missing so we have to find a solution to "clean" the data we have.
			</p>
			<p>
				The easiest option could be to eliminate all those patients with null/zero values, but in this way we would eliminate a lot of important data.
			</p>
			<p>
				Another option is to calculate the mean and the median value for a specific column and substitute that value everywhere (in the same column) we have zero or null. Let's see how to apply this second method.</p>
			</p>
			<div align="left" style="background-color:lightblue; padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					diabetes_copy['Glucose'].fillna(diabetes_copy['Glucose'].mean(), inplace = True)</br>
					diabetes_copy['BloodPressure'].fillna(diabetes_copy['BloodPressure'].mean(), inplace = True)</br>
					diabetes_copy['SkinThickness'].fillna(diabetes_copy['SkinThickness'].median(), inplace = True)</br>
					diabetes_copy['Insulin'].fillna(diabetes_copy['Insulin'].median(), inplace = True)</br>
					diabetes_copy['BMI'].fillna(diabetes_copy['BMI'].median(), inplace = True)</br>

				</code>
			</div>
			<h2>Feature Scaling</h2>
			<p>
				One of the most important data transformations we need to apply is the features scaling. Basically most of the machine learning algorithms don't work very well if the features have a different set of values. In our case for example the Age ranges from 20 to 80 years old, while the number of times a patient has been pregnant ranges from 0 to 17. For this reason we need to apply a proper transformation.
			</p>
			<p>
				data Z is rescaled such that Œº = 0 and ùõî = 1, and is done through this formula: z = (x - Œº) /ùõî
			</p>
			<div align="left" style="background-color:lightblue; padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					s = StandardScaler()</br>
					X =  pd.DataFrame(s.fit_transform(diabetes_copy.drop(["Outcome"],axis = 1),),</br>
					columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', </br>
						'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age'])</br>
					y = diabetes_copy.Outcome</br>
				</code>
			</div>
			<h2>Splitting the data in train, test and cross validation</h2>
			<p>
				<strong>Train Test Split:</strong> To have unknown datapoints to test the data rather than testing with the same points with which the model was trained. This helps capture the model performance much better.
			</p>
			<p>
				<strong>Cross Validation:</strong>  When model is split into training and testing it can be possible that specific type of data point may go entirely into either training or testing portion. This would lead the model to perform poorly. Hence over-fitting and underfitting problems can be well avoided with cross validation techniques
			</p>
			<div align="left" style="background-color:lightblue; padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</br>
				</code>
			</div>
			<h2>ML Models</h2>
			<h4>What is grid search?</h4>
			<p>
				Grid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.
			</p>
			<h4> ROC Curve</h4>
				<center><img src="roc_pima.jpg /></center>
			<p>
				AUC ROC is one of the most important evaluation metrics for any classification model‚Äôs performance.
			</p>

			<p>
				ROC (Receiver Operating Characteristic) Curve tells us about how good the model can distinguish between two things (e.g If a patient has a disease or no). Better models can accurately distinguish between the two. Whereas, a poor model will have difficulties in distinguishing between the two
			</p>
			<h4>Confusion Matrix</h4>
				<center><img src="roc_pima.jpg /></center>
			<p>
				A confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing. 
			</p>
			

			
			<p>
				<strong>Precision:</strong>  Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. 
			</p>
			<p>
				Precision = TP/TP+FP
			</p>
			<p>
				<strong>Recall (Sensitivity):</strong>   Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label? A recall greater than 0.5 is good. 
			</p>
			<p>
				Recall = TP/TP+FN
			</p>

			<p>
				<strong>F1 score:</strong>  F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it‚Äôs better to look at both Precision and Recall.
			</p>
			<p>
				F1 Score = 2(Recall Precision) / (Recall + Precision)
			</p>
			<h2>KNN</h2>
			<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					neighbors = [1,5,10,15,21,31,41,51]<br/>
					parameter = {'n_neighbors':neighbors}<br/>
					clf = KNeighborsClassifier(algorithm='brute')<br/>
					model = GridSearchCV(clf,parameter,verbose=1,scoring='roc_auc',n_jobs=-1,return_train_score=True)<br/>
					model.fit(X_train,y_train)<br/>
					opt_k = model.best_params_.get('n_neighbors')<br/>   
					train_pred = model.cv_results_.get('mean_train_score')<br/>
					cv_pred = model.cv_results_.get('mean_test_score')<br/>
					train_pred = train_pred.reshape(len(neighbors))<br/>
					cv_pred = cv_pred.reshape(len(neighbors)) <br/>
					plt.plot(neighbors, train_pred, 'b', label = "Train AUC")<br/>
					plt.plot(neighbors, cv_pred, 'r', label = "cross val")<br/>
					plt.legend()<br/>
					plt.ylabel('AUC score')<br/>
					plt.xlabel('k_hyperparameter')<br/>
					plt.show()<br/>
					print("opt_k is : ", opt_k)<br/>					

				</code>
			</div>
			<center><img src="roc_pima.jpg /></center>
			<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					model = KNeighborsClassifier(n_neighbors=opt_k, algorithm='brute')<br/>
					model.fit(X_train, y_train)<br/>
					tra_pred = model.predict_proba(X_train)[:,1]<br/>
					fpr, tpr, thresholds = roc_curve(y_train,tra_pred)<br/>
					roc_auc = auc(fpr, tpr)<br/>

					test_pred = model.predict_proba(X_test)[:,1]<br/>
					fpr2, tpr2, thresholds2 = roc_curve(y_test,test_pred)<br/>
					roc_auc2 = auc(fpr2, tpr2)<br/>

					plt.title('Receiver Operating Characteristic')<br/>
					plt.plot(fpr, tpr, 'b',label='Train AUC = %0.2f'% roc_auc)<br/>
					plt.plot(fpr2, tpr2, 'r',label='Test AUC = %0.2f'% roc_auc2)<br/>
					plt.legend(loc='lower right')<br/>
					plt.plot([0,1],[0,1],'g--')<br/>
					plt.ylabel('True Positive Rate')<br/>
					plt.xlabel('False Positive Rate')<br/>
					plt.show()<br/>					

				</code>
			</div>
			<center><img src="roc_pima.jpg /></center>
			<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					conf_matrix = confusion_matrix(y_train, model.predict(X_train))<br/>
					conf_matrix = confusion_matrix(y_test, model.predict(X_test))<br/>
					class_label = ['0', '1']<br/>
					df_conf_matrix = pd.DataFrame(conf_matrix, index=class_label, columns=class_label)<br/>
					sb.heatmap(df_conf_matrix, annot=True, fmt='d')<br/>
					plt.title("Confusion Matrix")<br/>
					plt.xlabel("Predicted")<br/>
					plt.ylabel("Actual")<br/>
					plt.show()<br/>		

				</code>
			</div>
			<center><img src="roc_pima.jpg /></center>
			<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					from sklearn.metrics import classification_report<br/>
					print("Classification Report: \n")<br/>
					prediction = model.predict(X_test)<br/>
					print(classification_report(y_test, prediction))<br/>	

				</code>
			</div>
			<div align="left" style="bpadding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					Classification Report: <br/>

              					precision    recall  f1-score   support<br/>

           				0       0.73      0.94      0.82        97<br/>
           				1       0.80      0.42      0.55        57<br/>

    				 accuracy                           0.75       154<br/>
   				macro avg       0.77      0.68      0.69       154<br/>
			     weighted avg       0.76      0.75      0.72       154<br/>


				</code>
			</div>
		</div>
	</body>
</html>
