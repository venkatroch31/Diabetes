<html>
	<head>
		<style>
			div {
  			padding-top: 10px;
  			padding-right:200px;
  			padding-bottom: 50px;
  			padding-left: 200px;  
			align:left;
			
			}
			
		</style>
	</head>
	
	<body>
		
		<div>
			<h1><center>Pima Indian Diabetes</center></h1>
			<h2>Introduction</h2>
			<p>
				The Pima are a group of Native Americans living in Arizona. A genetic predisposition allowed this group to survive normally to a diet poor of carbohydrates for years. In the recent years, because of a sudden shift from traditional agricultural crops to processed foods, together with a decline in physical activity, made them develop the highest prevalence of type 2 diabetes and for this reason they have been subject of many studies.
			</p>
			
			<p>
				Diabetes is a disease which occurs when the blood glucose level becomes high, which 
       				ultimately leads to other health problems such as heart diseases, kidney disease etc. 
       					Diabetes is caused mainly due to the consumption of highly processed food, bad 
       					consumption habits etc. According to WHO, the number of people with diabetes has 
       						been increased over the years. 

			</p>
			<h2>Dataset</h2>
			<p>
				Download the dataset from <a href="https://www.kaggle.com/uciml/pima-indians-diabetes-database/downloads/pima-indians-diabetes-database.zip/1">kaggle</a> 
			</p>
			<p>
				The dataset includes data from 768 women with 8 characteristics, in particular:
				<ul>
					<li>Pregnancies: Number of times pregnant</li>
					<li>Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test</li>
					<li>BloodPressure: Diastolic blood pressure (mm Hg)</li>
					<li>SkinThickness: Triceps skin fold thickness (mm)</li>
					<li>Insulin: 2-Hour serum insulin (mu U/ml)</li>
					<li>BMI: Body mass index (weight in kg/(height in m)^2)</li>
					<li>DiabetesPedigreeFunction: Diabetes pedigree function</li>
					<li>Age: Age (years)</li>
					<li>Outcome: Class variable (0 or 1)</li>
					
				</ul>
				The last column 'Outcome' of the dataset indicates if the person has been diagnosed with diabetes (1) or not (0)
			</p>
			<h2>Problem</h2>
			<p>
				The type of dataset and problem is a classic supervised binary classification. Given a number of elements all with certain characteristics (features), we want to build a machine learning model to identify people affected by type 2 diabetes.
			</p>
			<p>
				To solve the problem we will have to analyse the data, do any required transformation and normalisation, apply a machine learning algorithm, train a model, check the performance of the trained model and iterate with other algorithms until we find the most performant for our type of dataset.
			</p>
			<h2>Setup</h2>
			
			<p>
				Import neceessary libraries. Let‚Äôs begin:
			</p>
		
			<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					import pandas as pd</br>
					import matplotlib.pyplot as plt</br>
					import re</br>
					import numpy as np</br>
					import seaborn as sns</br>
					import time</br>
					import warnings</br>
					warnings.filterwarnings("ignore")</br>
					from sklearn.neighbors import KNeighborsClassifier</br>
					from sklearn.metrics.classification import accuracy_score, log_loss</br>
					from sklearn.svm import SVC</br>
					from sklearn.calibration import CalibratedClassifierCV</br>
					from sklearn.model_selection import train_test_split</br>
					from sklearn.model_selection import GridSearchCV</br>
					from sklearn.ensemble import RandomForestClassifier</br>
					from sklearn import metrics</br>

				</code>
			</div>
			<h2>Load the dataset</h2>
			<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					diabetes = pd.read_csv('diabetes.csv')</br>
					print('\nFeatures : ', diabetes.columns.values)</br>
					Features :  ['Pregnancies' 'Glucose' 'BloodPressure' 'SkinThickness' 'Insulin' 'BMI'
 						'DiabetesPedigreeFunction' 'Age' 'Outcome']</br>
					

				</code>
			</div>



			<h2>Data correlation matrix</h2>
			<p>
				The correlation matrix is an important tool to understand the correlation between the different characteristics. The values range from -1 to 1 and the closer a value is to 1 the bettere correlation there is between two characteristics. Let's calculate the correlation matrix for our dataset.
			</p>
			<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					plt.figure(figsize=(12,10))</br>
					p=sns.heatmap(diabetes.corr(), annot=True,cmap ='RdYlGn')  </br>
					

				</code>
			</div>
			
				<center><img src="1.PNG" /></center>
			
			<h2>Visualise the Data</h2>
			<p>
				Visualising the data is an important step of the data analysis. With a graphical visualisation of the data we have a better understanding of the various features values distribution: for example we can understand what's the average age of the people or the average BMI etc...
			</p>
			<center><img src=""/></center>
			<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					df = diabetes.hist(figsize=(12,10))</br>					

				</code>
			</div>
			
				<center><img src="2.PNG" /></center>
			
				<p>
					An important thing I notice in the dataset is the fact that some people have null (zero) values for some of the features: it's not quite possible to have 0 as BMI or for the blood pressure.
				</p>
			
				<p>
					The following columns or variables have an invalid zero value:
					<ul>
						<li>Glucose</li>
						<li>BloodPressure</li>
						<li>SkinThickness</li>
						<li>Insulin</li>
						<li>BMI</li>
					</ul>
					
				</p>
				<p>
					
					It is better to replace zeros with nan since after that counting them would be easier and zeros need to be replaced with suitable values
					
				</p>
				<div align="left" style="background-color:lightblue; padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
					<code>
					
						diabetes_copy = diabetes.copy(deep = True)</br>
						diabetes_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)
						</br>
					
						print(diabetes_copy.isnull().sum())</br>					

					</code>
				</div>
				
				<div align="left" style="padding-left:10px" style="overflow-y:hidden;overflow-x:scroll">
					<code>
					
						Pregnancies                   0</br>
						Glucose                       5</br>					
						BloodPressure                35</br>
						SkinThickness               227</br>
						Insulin                     374</br>
						BMI                          11</br>
						DiabetesPedigreeFunction      0</br>
						Age                           0</br>
						Outcome                       0</br>
						dtype: int64

					</code>
				</div>

			<h2>Data cleaning and transformation</h2>
			<p>
				We have noticed from the previous analysis that some patients have missing data for some of the features. Machine learning algorithms don't work very well when the data is missing so we have to find a solution to "clean" the data we have.
			</p>
			<p>
				The easiest option could be to eliminate all those patients with null/zero values, but in this way we would eliminate a lot of important data.
			</p>
			<p>
				Another option is to calculate the median value for a specific column and substitute that value everywhere (in the same column) we have zero or null. Let's see how to apply this second method.</p>
			</p>
			<div align="left" style="background-color:lightblue; padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					diabetes_copy['Glucose'].fillna(diabetes_copy['Glucose'].mean(), inplace = True)</br>
					diabetes_copy['BloodPressure'].fillna(diabetes_copy['BloodPressure'].mean(), inplace = True)</br>
					diabetes_copy['SkinThickness'].fillna(diabetes_copy['SkinThickness'].median(), inplace = True)</br>
					diabetes_copy['Insulin'].fillna(diabetes_copy['Insulin'].median(), inplace = True)</br>
					diabetes_copy['BMI'].fillna(diabetes_copy['BMI'].median(), inplace = True)</br>

				</code>
			</div>
			<h2>Feature Scaling</h2>
			<p>
				One of the most important data transformations we need to apply is the features scaling. Basically most of the machine learning algorithms don't work very well if the features have a different set of values. In our case for example the Age ranges from 20 to 80 years old, while the number of times a patient has been pregnant ranges from 0 to 17. For this reason we need to apply a proper transformation.
			</p>
			<p>
				data Z is rescaled such that Œº = 0 and ùõî = 1, and is done through this formula: z = (x - Œº) /ùõî
			</p>
			<div align="left" style="background-color:lightblue; padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>
					
					s = StandardScaler()</br>
					X =  pd.DataFrame(s.fit_transform(diabetes_copy.drop(["Outcome"],axis = 1),),</br>
					columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', </br>
						'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age'])</br>
					y = diabetes_copy.Outcome</br>
				</code>
			</div>
			<h2>Splitting the data </h2>
			<p>
				We transformed the data then we need to split the dataset in two , then we use the trained model to make predictions on new data (which is the test dataset, not part of the training set) and compare the predicted value with the pre assigned label.
			</p>
			<p>
				<strong>Cross Validation:</strong>  When model is split into training and testing it can be possible that specific type of data point may go entirely into either training or testing portion. This would lead the model to perform poorly. Hence over-fitting and underfitting problems can be well avoided with cross validation techniques
			</p>
			<div align="left" style="background-color:lightblue; padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>					
					X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</br>
				</code>
			</div>
			<h2>ML Models</h2>
			<div align="left" style="background-color:lightblue; padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>					
					
					models = []<br/>
					models.append(('LR', LogisticRegression()))<br/>
					models.append(('KNN', KNeighborsClassifier()))<br/>
					models.append(('NB', GaussianNB()))<br/>
					models.append(('SVC_lin', SVC(kernel="linear")))<br/>
					models.append(('SVC_rbf', SVC(kernel="rbf")))<br/>
					models.append(('LSVC', LinearSVC()))<br/>
					models.append(('RFC', RandomForestClassifier()))<br/>
					models.append(('DTR', DecisionTreeRegressor()))<br/>
					models.append(('XGB', XGBClassifier()))<br/>
				</code>
			</div><br/><br/>
			<div align="left" style="background-color:lightblue; padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>					
					
					# Prepare the configuration to run the test<br/>
					seed = 9<br/>
					results = []<br/>
					names = []<br/>
					X = X_train<br/>
					Y = y_train<br/>
				</code>
			</div>
			<div align="left" style="background-color:lightblue; padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>					
					
					for name, model in models:<br/>
   					 	kfold = model_selection.KFold(n_splits=10, random_state=seed)<br/>
    						cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')<br/>
    						results.append(cv_results)<br/>
    						names.append(name)<br/>
    						msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())<br/>
    						print(msg)<br/>
				</code>
			</div>
			<div align="left" style="background-color:lightblue; padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>					
					
					for name, model in models:<br/>
   					 	kfold = model_selection.KFold(n_splits=10, random_state=seed)<br/>
    						cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')<br/>
    						results.append(cv_results)<br/>
    						names.append(name)<br/>
    						msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())<br/>
    						print(msg)<br/>
				</code>
			</div>
			div align="left" style=" padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>					
					
					
					LR: 0.763934 (0.062926)<br/>
					KNN: 0.744236 (0.044908)<br/>
					NB: 0.744157 (0.052273)<br/>
					SVC_lin: 0.768747 (0.060924)<br/>
					SVC_rbf: 0.754098 (0.058642)<br/>
					LSVC: 0.760682 (0.064072)<br/>
					RFC: 0.734320 (0.071838)<br/>
					DTR: 0.692147 (0.080603)<br/>
					XGB: 0.760471 (0.052592)<br/>
				</code>
			</div>
			div align="left" style=" background-color:lightblue;padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>					
					
					
					
					# boxplot algorithm comparison<br/>
					fig = plt.figure()<br/>
					fig.suptitle('Algorithm Comparison')<br/>
					ax = fig.add_subplot(111)<br/>
					plt.boxplot(results)<br/>
					ax.set_xticklabels(names)<br/>
					plt.show()<br/>
				</code>
			</div>
			<h4>What is grid search?</h4>
			<p>
				Grid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.
			</p>
			
		
			div align="left" style=" background-color:lightblue;padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>				
					param_grid = {'C': [0.01, 0.1, 1.0, 10.0, 50.0],'kernel': ['linear']}<br/>
					model_svc = SVC()<br/>
					grid_search = GridSearchCV(model_svc, param_grid, cv=10, scoring='accuracy')<br/>
					grid_search.fit(X_train, y_train)<br/>
					
					
					
				</code>
			</div>
			div align="left" style=" background-color:lightblue;padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>				
					# Print the bext score found and best_estimator<br/>
					print("Best score: %0.2f%%" % (100*grid_search.best_score_))<br/>
					print("Best estimator for parameter C: %f" % (grid_search.best_estimator_.C))<br/>
					
					
					
				</code>
			</div>
			div align="left" style="padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>				
					Best score: 77.36%<br/>
					Best estimator for parameter C: 0.010000<br/>				
					
					
				</code>
			</div>
			<h2>Apply the parameters to the model and train it</h2>
			div align="left" style="background-color:lightblue;padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>				
					# Create an instance of the algorithm using parameters<br/>
					# from best_estimator_ property<br/>
					svc = grid_search.best_estimator_<br/>

					# Use the whole dataset to train the model<br/>
					X = np.append(X_train, X_test, axis=0)<br/>
					Y = np.append(y_train, y_test, axis=0)<br/>

					# Train the model<br/>
					svc.fit(X, Y)<br/>				
					
					
				</code>
			</div>

			div align="left" style="padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>				
					
					SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,<br/>
    						decision_function_shape='ovr', degree=3, gamma='auto_deprecated',<br/>
    						kernel='linear', max_iter=-1, probability=False, random_state=None,<br/>
    						shrinking=True, tol=0.001, verbose=False)<br/>				
					
					
				</code>
			</div>
			<h2>Make a Prediction</h2>
			div align="left" style="background-color:lightblue;padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>				
					
					
					# We create a new (fake) person having the three most correated values high<br/>
					new_df = pd.DataFrame([[16, 168, 62, 45, 0, 23.6, 0.62, 65]])<br/>
					# We scale those values like the others<br/>
					new_df_scaled = s.transform(new_df)<br/>
					prediction = svc.predict(new_df_scaled)<br/>
					# A value of "1" means that this person is likley to have type 2 diabetes<br/>
					prediction<br/>
					
					
					
				</code>
			</div><br/>
			div align="left" style="padding-left:10px"; style="overflow-y:hidden;overflow-x:scroll">
				<code>				
					
					
					array([1])
					
					
					
				</code>
			</div>

			
			
			
			
			
			<h2>Conclusion</h2>
			<p>
				We finally find a score of 76% using SVC algorithm and parameters optimisation. Please note that there may be still space for further analysis and optimisation, for example trying different data transformations or trying algorithms that haven't been tested yet. Once again I want to repeat that training a machine learning model to solve a problem with a specific dataset is a try / fail / improve process.
			</p>
		</div>
	</body>
</html>
