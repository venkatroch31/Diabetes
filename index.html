<html>
  <head>
    <style>
h1 {
  color: blue;
  font-family: verdana;
  font-size: 300%;
}
h2 {
  color: black;
  font-family: verdana;
  font-size: 170%;
}
p {
  color: black;
  font-family: verdana;
  font-size: 100%;
}
ul {
  color: black;
  font-family: verdana;
  font-size: 90%;
}
  div {
  padding-top: 10px;
  padding-right: 40px;
  padding-bottom: 50px;
  padding-left: 100px;
}
      
 a {
 
   background-color: lightgreen;
}
      tab {
    display: inline-block; 
    margin-left: 40px; 
}
</style>
  </head>
  
  
<body><div>
 
  </center>
  <h1><center>Machine Learning for pima Indian Diabetes</center></h1>
 
  <h2>Introduction</h2>
  <h4> What is Diabetes?</h4>
  <p>  Diabetes is a disease which occurs when the blood glucose level becomes high, which 
       ultimately leads to other health problems such as heart diseases, kidney disease etc. 
       Diabetes is caused mainly due to the consumption of highly processed food, bad 
       consumption habits etc. According to WHO, the number of people with diabetes has 
       been increased over the years. 
  </p>
  <p>The Pima are a group of Native Americans living in Arizona. A genetic predisposition 
    allowed this group to survive normally to a diet poor of carbohydrates for years. In the 
    recent years, because of a sudden shift from traditional agricultural crops to processed 
    foods, together with a decline in physical activity, made them develop the highest prevalence
    of type 2 diabetes and for this reason they have been subject of many studies. 
    
    
  </p>
  <h2>Description </h2>
  <p>
    <strong>Source:</strong> https://www.kaggle.com/uciml/pima-indians-diabetes-database/downloads/pima-indians-diabetes-database.zip/1 </p>
  <p>
    <strong>Data Info:</strong>
    When encountered with a data set, first we should analyse and "get to know" the data set. 
    This step is necessary to familiarize with the data, to gain some understanding about the 
    potential features and to see if data cleaning is needed.
  </p>
   <p>
    <strong>Problem statement :</strong>
 
    The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, 
    based on certain diagnostic measurements included in the dataset. 
  </p>
  
  
  
  <h2>Prerequisites</h2>
  <ul>
         <li>Python 3</li>
         <li>Anaconda (Scikit Learn, Numpy, Pandas, Matplotlib, Seaborn)</li>
         <li>Jupyter Notebook</li>
         <li>Basic understanding of supervised machine learning methods : specifically classification</li>
      </ul>



<h2> Data </h2>
  <p>First import neceessary libraries. When encountered with a data set, first we should analyse and get to know the data set. To get some understanding about
    the potential features and to see if data cleaning is needed.</p>
  <a>
    import pandas as pd<br/>
    
    import numpy as np<br/>
    
    import matplotlib.pyplot as plt<br/>
    
    import seaborn as sns<br/>
    
    diabetes = pd.read_csv('diabetes.csv')
    </a>
  
 <center><img src="22.PNG" alt=" "> </center>
<p>
  Outcome is the feature we are going to predict, 0 means no diabetes, 
  1 means diabetes, these 768 data points, 500 are labeled as 0 and 268 as 1
</p>
  <a>sns.countplot(diabetes['Outcome'],label="Count") </a>
<center><img src="vis0.png" alt=" "> </center>
  <p>
    Visualization of data is an imperative aspect of data science. It helps to understand data and also 
    to explain the data to another person. Python has several interesting visualization libraries such as
    Matplotlib, Seaborn etc.In this tutorial we will use pandas’ visualization which is built on top of 
    matplotlib, to find the data distribution of the features.

  </p>
  <a>df = diabetes.hist(figsize=(12,10))</a>
  
<center>
  <img src="vis.png" alt=" "> </center>
<ul>
         <li>Pregnancies: Number of times pregnant</li>
         <li>Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test</li>
         <li>BloodPressure: Diastolic blood pressure (mm Hg)</li>
         <li>SkinThickness: Triceps skin fold thickness (mm)</li>
          <li>Insulin: 2-Hour serum insulin (mu U/ml)</li>
          <li>BMI: Body mass index (weight in kg/(height in m)^2)</li>
          <li>DiabetesPedigreeFunction: Diabetes pedigree function</li>
          <li>Age: Age (years)</li>
          <li>Outcome: Class variable (0 or 1)</li>
      </ul>
  <h2>Preprocessing of Data</h2>
  <p>
    Considered to be one of the crucial steps of the work flow, because it can make or break the model. 
    There is a saying in machine learning Better data beats fancier algorithms, which suggests better 
    data gives you better resulting models.
    There are several factors to consider in the data cleaning process:
     <ul>
         <li>Duplicate or irrelevant observations.</li>
         <li>Bad labeling of data, same category occurring multiple times.</li>
         <li>Missing or null data points.</li>
         
      </ul>

    
  </p>
  
  <h2>Feature Engineering </h2>
  <p>
    Feature engineering is the process of transforming the gathered data into features that better represent the 
    problem that we are trying to solve to the model, to improve its performance and accuracy.Feature engineering
    create more input features from the existing features and also combine several features to produce more intuitive 
    features to feed to the model.
  </p>
  <p>
    The domain of the problem we are trying to tackle requires lots of related features. Since the data set is already 
    provided, and by examining the data we can’t further create or dismiss any data at this point. In the data set we 
    have the following 8 features.

    (Pregnancies, Glucose, Blood Pressure, Skin Thickness, Insulin, BMI, Diabetes Pedigree Function, Age)

    By the observation we can say that the 'Skin Thickness' is not an indicator of diabetes. But we can not deny the 
    fact that it is unusable at this point. Therefore we will use all the features available. We separate the data set 
    into features and the response that we are going to predict. We will assign the features to the X variable and the
    response to the y variable.
  </p>
  <a>
    X = diabetes.drop(diabetes.columns[8:],axis=1)<br/>
    y = diabetes['Outcome']
  </a>
  <h2>Train, CV and Test Datasets</h2>
  <p>
    Split the dataset randomly into three parts train, cross validation and test with 64%,16%, 20% of data respectively
    </p>
  <a>
    from sklearn.model_selection import train_test_split<br/>
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)<br/>
    X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.2)
  </a>
  <h2>Machine Learning Models</h2>
  <h2>Model Selection</h2>
  <p>
    Model selection or algorithm selection phase is the most exciting and the heart of machine learning. 
    It is the phase where we select the model which performs best for the data set at hand.
  </p>
  <p>
    We will import the necessary libraries to the notebook. We import 7 classifiers namely K-Nearest Neighbors, 
    Support Vector Classifier, Logistic Regression, MultiNomial Naive Bayes, Random Forest and  XgBoost to be contenders 
    for the best classifier.
  </p>
  <a>
    from sklearn.neighbors import KNeighborsClassifier<br/>
from sklearn.calibration import CalibratedClassifierCV<br/>
from sklearn.naive_bayes import MultinomialNB<br/>
from sklearn.ensemble import RandomForestClassifier<br/>
from sklearn.linear_model import SGDClassifier<br/>
from sklearn.linear_model import LogisticRegression<br/>
from sklearn.linear_model import LogisticRegression

  </a>
  <h2>Evaluation Methods</h2>
   <p>The Log Loss metric takes into account the probabilities underlying your models, and 
    not only the final output of the classification. The bolder the probabilities, the better
    will be your Log Loss — closer to zero. It is a measure of uncertainty, so a low Log Loss 
    means a low uncertainty/entropy of your model. Log Loss is similar to the Accuracy, but it 
    will favor models that distinguish more strongly the classes. Log Loss it useful to compare 
     models not only on their output but on their probabilistic outcome.</p>
  <a>
    
from sklearn.metrics import confusion_matrix<br/>
from sklearn.metrics.classification import accuracy_score, log_loss<br/>
from sklearn.metrics import precision_recall_curve, auc, roc_curve
  </a>
  
  
  <h2>k-Nearest Neighbors</h2>
  <p>
    The k-NN algorithm is arguably the simplest machine learning algorithm. Building the model 
    consists only of storing the training data set. To make a prediction for a new data point, 
    the algorithm finds the closest data points in the training data set — its "nearest neighbors"
  </p>
  <a>
 alpha = [5, 11, 15, 21, 31, 41, 51, 99]<br/>
cv_log_error_array = []<br/>
for i in alpha:<br/>
    <tab>print("for alpha =", i)<br/>
    clf = KNeighborsClassifier(n_neighbors=i)<br/>
    clf.fit(X_train, y_train)<br/>
    sig_clf = CalibratedClassifierCV(clf, method="sigmoid")<br/>
    sig_clf.fit(X_train, y_train)<br/>
    sig_clf_probs = sig_clf.predict_proba(X_cv)<br/>
    cv_log_error_array.append(log_loss(y_cv, sig_clf_probs, labels=clf.classes_, eps=1e-15))<br/>
    # to avoid rounding error while multiplying probabilites we use log-probability estimates<br/>
    print("Log Loss :",log_loss(y_cv, sig_clf_probs)) </tab>
    <br/>
fig, ax = plt.subplots()<br/>
ax.plot(alpha, cv_log_error_array,c='g')<br/>
for i, txt in enumerate(np.round(cv_log_error_array,3)):<br/>
    <tab>  ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))<br/></tab>
plt.grid()<br/>
plt.title("Cross Validation Error for each alpha")<br/>
plt.xlabel("Alpha i's")<br/>
plt.ylabel("Error measure")<br/>
plt.show()
  </a>
<h2>Naive Bayes</h2>
<p>
  Naive Bayes is one of the most common classification algorithms.
</p>
<h2>Logistic Regresion</h2>
<p>
  Logistic Regresion is one of the most common classification algorithms.
</p>
<h2>Linear SVM</h2>
<p>
  Linear SVM is one of the most common classification algorithms.
</p>
<h2>Random Forest</h2>
<p>
  Random Forest is one of the most common classification algorithms.
</p>
<h2>XgBoost</h2>
<p>
  XgBoost is one of the most common classification algorithms.
</p>
  </div>
</body>
</html>
